---
title: "Final Project"
output: pdf_document
date: "2024-04-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(summarytools)
library(corrplot)
library(gt)
library(caret)
library(glmnet)
library(rpart)
library(rpart.plot)
library(ggfortify)
```

## Loading the Data

```{r}
stroke <- read.csv("https://raw.githubusercontent.com/Mattr5541/DATA-621-Final-Project/main/621clean_shout_dat(in).csv")
```

## Data Cleaning

First, I will code all "Unknown" observations as NA, as their presence may confound our analysis

```{r}
stroke <- stroke %>% mutate(across(where(is.character), ~na_if(., "Unknown")))
stroke <- stroke %>% mutate(across(where(is.character), ~na_if(., "Unknown or Not Reported")))
```

## Recoding NA values with N where applicable

Certain variables were not coded with N where the presence of an outcome was false. This can be seen in variables with only one outcome

```{r}
#unique_values <- lapply(stroke, unique)
#print(unique_values)

stroke <- stroke %>% 
  mutate(
    CovidAtVisitFlag = replace_na(CovidAtVisitFlag, 'N'),
    FamilyHistoryStrokeFlag = replace_na(FamilyHistoryStrokeFlag, 'N'),
    prior.COVID.19 = replace_na(prior.COVID.19, 'N'),
    hypertension = replace_na(hypertension, 'N'),
    diabetes.mellitus = replace_na(diabetes.mellitus, 'N'),
    diabetes.mellitus.type.2 = replace_na(diabetes.mellitus.type.2, 'N'),
    myocardial.infarction = replace_na(myocardial.infarction, 'N'),
    alzheimer.s.disease = replace_na(alzheimer.s.disease, 'N'),
    hyperlipidemia = replace_na(hyperlipidemia, 'N'),
    atrial.fibrillation = replace_na(atrial.fibrillation, 'N'),
    chronic.heart.disease = replace_na(chronic.heart.disease, 'N'),
    chronic.kidney.disease = replace_na(chronic.kidney.disease, 'N'),
    carotid.stenosis = replace_na(carotid.stenosis, 'N'),
    Coronary.artery.disease = replace_na(Coronary.artery.disease, 'N'),
    Heart.failure = replace_na(Heart.failure, 'N'),
    Peripheral.vascular.disease = replace_na(Peripheral.vascular.disease, 'N'),
    Dysphagia_outcome = replace_na(Dysphagia_outcome, 'N'),
    ispregnancyDoc = replace_na(ispregnancyDoc, 'N'), 
    ispregnancyICD = replace_na(ispregnancyICD, 'N'),
    isTransferEvent = replace_na(isTransferEvent, 'N'))
```


### Examining Missingness

My next step wil be to remove columns that present 80% or more missingness, as they will likely not contribute to our analyses, and any attempts to impute values for these columns may generate unreliable data (we may have to consider the same for columns that present 50% or more missing values)

```{r}
miss_percent <- colSums(is.na(stroke) / 29662 * 100)

miss_percent_80 <- as.data.frame(miss_percent)

miss_percent_80 <- miss_percent_80 %>% filter(miss_percent > 79)

print(miss_percent_80)
```

## Exploratory Data Analysis

```{r}
stroke <- stroke %>% 
  select(-alcohol_use_frequency, -evt, -evt_status, -tici_score)

```

## Splitting the Data into Training/Test Sets

Before modifying the dataset any further, I will split the data into train/test partitions for the purposes of model validation (I will use a standard 80/20 split. To start, however, I want to see how evenly the binary outcomes of our target variable occur in our dataset (I'm assuming there will be an uneven split that is more biased toward negative outcomes)

```{r}
table(stroke$TARGET)
```

As expected, there is a bias toward negative outcomes, presenting the issue of imbalance in our data. As a result, we may need to perform an oversampling or undersampling procedure to account for this, or otherwise balance observations while constructing our models.

```{r}
set.seed(12345)

train_test <- createDataPartition(stroke$TARGET, p = 0.8, list = F)

stroke_train <- stroke[train_test, ]
stroke_test <- stroke[-train_test, ]
```

## Exploratory Data Analysis

### Frequencies

```{r}
stroke_train_cat <- select_if(stroke_train, is.character)

stroke_freq <- dfSummary(stroke_train_cat, stats = 'freq')

view(stroke_freq)
```

### Descriptive Statistics

```{r}
stroke_train_quant <- select_if(stroke_train, is.numeric)

stroke_train_quant <- stroke_train_quant %>% select(-IsIschaemicStrokeEvent) #Removing because the only value is 1

stroke_sum <- dfSummary(stroke_train_quant, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(stroke_sum)
```

### Correlation Matrix

```{r}
cor_matrix = cor(stroke_train_quant, use = "complete.obs")

print(cor_matrix)

corrplot(cor_matrix, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black")
```

As we can see, most of the correlations present are rather weak. The exception would be the correlations among Arrival_NHISS_score and the cleaned version (I will drop the original), and some moderate correlations among MRS_discharge_score_cleaned and the NHISS scores. Aside from that, there seems to be no real concern regarding multicollinearity among these variables.

Interestingly, there seem to be no high correlations among the predictors and target variables, suggesting that our features may be weak predictors, by themselves, of recurrent strokes, which is rather interesting, since these factors should intuitively be related to the presence of recurrent strokes.

```{r}
stroke_train <- stroke_train %>% select(-Arrival_NIHSS_score)
stroke_test <- stroke_test %>% select(-Arrival_NIHSS_score)
```

# Data Handling and Cleaning
## Missing data

```{r}
missing_percentage <- stroke_train %>%
  summarise_all(~ mean(is.na(.)) * 100)

print(missing_percentage)
```

The following variables have missing data. I broke them up based on the type and provided the percent of missing data to inform the best method to impute the missing data.

Continuous Variables:
Length_of_stay_hours (<1%), MRS_discharge_score_cleaned (14%),
Arrival_NIHSS_score_cleaned (25%), BMI(18.7%). These variables are not normally distributed so I will use median imputation

Categorical or Ordinal Variables: Race, Ethnicity, Gender, Arrival_mode, Arrival_from, Discharge_disposition, Visit_data_dispo, Tobacco_current_use_indicator, Tobacco_prior_use_indicator, InsuranceCategory, Discharge_disposition_regex. To preserve the nature of these variables, I will use mode imputation as it replaces missing values with the most frequent category. 

```{r}
mode_impute <- function(x) {
  mode_val <- names(sort(table(x), decreasing = TRUE))[1]
  x[is.na(x)] <- mode_val
  return(x)
}

columns_to_impute <- c("race", "ethnicity", "gender", "arrival_mode", "arrival_from", "discharge_disposition", "visit_data_dispo", "Tobacco_current_use_indicator", "Tobacco_prior_use_indicator", "InsuranceCategory", "discharge_disposition_regex")

stroke_train <- stroke_train %>%
  mutate_at(.vars = columns_to_impute, .funs = mode_impute)

stroke_train$MRS_discharge_score_cleaned <- ifelse(
    is.na(stroke_train$MRS_discharge_score_cleaned), 
    median(stroke_test$MRS_discharge_score_cleaned, na.rm = TRUE), 
    stroke_train$MRS_discharge_score_cleaned
)

stroke_train$Arrival_NIHSS_score_cleaned <- ifelse(
    is.na(stroke_train$Arrival_NIHSS_score_cleaned), 
    median(stroke_test$Arrival_NIHSS_score_cleaned, na.rm = TRUE), 
    stroke_train$Arrival_NIHSS_score_cleaned
)

stroke_train$Length_of_stay_hours <- ifelse(
    is.na(stroke_train$Length_of_stay_hours), 
    median(stroke_test$Length_of_stay_hours, na.rm = TRUE), 
    stroke_train$Length_of_stay_hours
)

stroke_train$BMI <- ifelse(
    is.na(stroke_train$BMI), 
    median(stroke_test$BMI, na.rm = TRUE), 
    stroke_train$BMI
)
```

Now there is no missing data in stroke_train dataset

```{r}
missing_data_report = stroke_train %>%
  summarise_all(~sum(is.na(.)))

print(missing_data_report)
```



```{r}
#imputing testing dataset
mode_impute <- function(x) {
  mode_val <- names(sort(table(x), decreasing = TRUE))[1]
  x[is.na(x)] <- mode_val
  return(x)
}
columns_to_impute <- c("race", "ethnicity", "gender", "arrival_mode", "arrival_from", "discharge_disposition", "visit_data_dispo", "Tobacco_current_use_indicator", "Tobacco_prior_use_indicator", "InsuranceCategory", "discharge_disposition_regex")

stroke_test <- stroke_test %>%
  mutate_at(.vars = columns_to_impute, .funs = mode_impute)

stroke_test$MRS_discharge_score_cleaned <- ifelse(is.na(stroke_test$MRS_discharge_score_cleaned), median(stroke_test$MRS_discharge_score_cleaned, na.rm = TRUE), stroke_test$MRS_discharge_score_cleaned)
stroke_test$Arrival_NIHSS_score_cleaned <- ifelse(is.na(stroke_test$Arrival_NIHSS_score_cleaned), median(stroke_test$Arrival_NIHSS_score_cleaned, na.rm = TRUE), stroke_test$Arrival_NIHSS_score_cleaned)
stroke_test$Length_of_stay_hours <- ifelse(is.na(stroke_test$Length_of_stay_hours), median(stroke_test$Length_of_stay_hours, na.rm = TRUE), stroke_test$Length_of_stay_hours)
stroke_test$BMI <- ifelse(is.na(stroke_test$BMI), median(stroke_test$BMI, na.rm = TRUE), stroke_test$BMI)
```

```{r}
missing_data_test = stroke_test%>%
  summarise_all(~sum(is.na(.)))

print(missing_data_test)
```

###Dummy Coding Categorical Variables

Creating dummy coding for categorical variables, in both training and testing datasets, results in a format that helps prepare data for further analysis. The '-1' part of the code was done to avoid multicollinearity issues.

```{r}
# Function to create dummy variables with consistent naming
create_dummies <- function(data, variable_name) {
  dummies <- model.matrix(~ get(variable_name) - 1, data=data)
  colnames(dummies) <- paste("dummy", variable_name, gsub("(Intercept)|get\\(variable_name\\)", "", colnames(dummies)), sep="_")
  return(dummies)
}

# List of categorical variables
variables_list <- c("race", "ethnicity", "gender", "vital_status", "age_group", "visit_type",
                    "Tobacco_current_use_indicator", "Tobacco_prior_use_indicator", 
                    "FamilyHistoryStrokeFlag", "hypertension", "diabetes.mellitus", 
                    "diabetes.mellitus.type.2", "myocardial.infarction", "hyperlipidemia", 
                    "atrial.fibrillation", "chronic.heart.disease", "chronic.kidney.disease", 
                    "Coronary.artery.disease", "Heart.failure", "Dysphagia_outcome", 
                    "isTransferEvent")

# Apply the function to both datasets using a loop to create dummy variables
for (var in variables_list) {
  stroke_train[paste("dummy", var, sep="_")] <- create_dummies(stroke_train, var)
  stroke_test[paste("dummy", var, sep="_")] <- create_dummies(stroke_test, var)
}

```




## Transformation 

Log transformation on the variable BMI should prove to be helpful since the range of 2 to 259 is unrealistic in real world metrics (on both the higher and smaller end). The same transformation on Length_of_stay_hours would also be useful as there likely should not be negative hours nor 9,666 hours (max value) which estimates to over a year.
```{r}
stroke_train[] <- lapply(stroke_train, function(x) {
    if(is.factor(x)) factor(x) else x
})



stroke_train$log_BMI <- log(stroke_train$BMI + 1)
stroke_train$log_Length_of_stay_hours <- log(stroke_train$Length_of_stay_hours + 1)

stroke_test$log_BMI <- log(stroke_test$BMI + 1)
stroke_test$log_Length_of_stay_hours <- log(stroke_test$Length_of_stay_hours + 1)

print(sum(is.na(stroke_train$log_BMI)))
print(sum(is.na(stroke_train$log_Length_of_stay_hours)))

print(sum(is.na(stroke_test$log_BMI)))
print(sum(is.na(stroke_test$log_Length_of_stay_hours)))



train_stats <- dfSummary(stroke_train, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(train_stats)

```




```{r}


# Check the histograms of the log-transformed variables
par(mfrow=c(1,2))
hist(stroke_train$log_BMI, main = "Log-transformed BMI")
hist(stroke_train$log_Length_of_stay_hours, main = "Log-transformed Length_of_stay_hours")


```

### Building the Model

#### Filtering out all the categorical variables

```{r}
stroke_train = stroke_train %>%
  select(where(~!is.character(.)))

stroke_test = stroke_test %>%
  select(where(~!is.character(.)))
```

#### Logistic Regression:

```{r}
model <- glm(TARGET ~ ., 
             data = stroke_train, 
             family = binomial)

summary(model)
```

The summary yields the result "Coefficients: (28 not defined because of singularities)." In an effort to counter this negative result, I will perform both Ridge and Lasso Regressions.

##### Lasso:

```{r}
# Extract predictor variables and the target variable from the stroke_train data
# Impute the missing value using the median of the column
#stroke_train$log_Length_of_stay_hours[is.na(stroke_train$log_Length_of_stay_hours)] <- median(stroke_train$log_Length_of_stay_hours, na.rm = TRUE)

stroke_train[] <- lapply(stroke_train, function(x) {
    if(is.factor(x)) factor(x) else x
})
# Impute the missing value using the median of the column
stroke_train$log_Length_of_stay_hours[is.na(stroke_train$log_Length_of_stay_hours)] <- median(stroke_train$log_Length_of_stay_hours, na.rm = TRUE)

X <- model.matrix(~ . - 1 - TARGET, data = stroke_train)  # Remove intercept term
Y <- stroke_train$TARGET

# Fit the Lasso logistic regression model
lasso_model <- glmnet(X, Y, family = "binomial", alpha = 1)  # Set alpha = 1 for Lasso penalty

# Get the lambda values from the Lasso model
lambda_values <- lasso_model$lambda

# Perform cross-validation
cv_model <- cv.glmnet(X, Y, alpha = 1, lambda = lambda_values, nfolds = 10)

# Select optimal lambda
optimal_lambda <- cv_model$lambda.min
print(paste("Optimal Lambda:", optimal_lambda))

# Refit final model
lasso_model <- glmnet(X, Y, family = "binomial", alpha = 1, lambda = optimal_lambda)

print(lasso_model)
```

Lasso Regression is helpful here as the variable selection is almost "automatic" and given that the dataset has a lot of predictors, this is extremely valuable. Lasso regression also helps with possible over-fitting as well.

##### Ridge Regression:

```{r}
# Extract predictor variables and the target variable from the stroke_train data
X <- model.matrix(~ . - 1 - TARGET, data = stroke_train)  # Remove intercept term
Y <- stroke_train$TARGET

# Perform cross-validation to select optimal lambda value
cv_model <- cv.glmnet(X, Y, family = "binomial", alpha = 0, type.measure = "deviance")

# Select optimal lambda
optimal_lambda <- cv_model$lambda.min
print(paste("Optimal Lambda:", optimal_lambda))

# Fit the ridge logistic regression model with the selected lambda value
ridge_model <- glmnet(X, Y, family = "binomial", alpha = 0, lambda = optimal_lambda)

# Summary of the ridge logistic regression model
print(ridge_model)
```

Ridge regression is also a great method for "removing" the affect caused by irrelevant predictors in the dataset and therefore in the model. Though, the difference with Ridge regression is that Ridge regression does not "remove" the more irrelevant predictors, which could help with multicollinearity in a more graceful manner.

#### Decision Trees

```{r}
tree_model <- rpart(TARGET ~ ., data = stroke_train, method = "class")

rpart.plot(tree_model, type = 4, extra = 101)
```

Given that the goal is predict a binary "TARGET" variable with many different variables, using a decision tree may be an advantageous method given that decision trees automatically select the most relevant variables.

### Model Validation and Selection

I will use mean standard error to conduct model validation.

```{r}
# Logistic Regression
predicted_vals_lr <- predict(model, newdata = stroke_train, type = "response")
observed_vals_lr <- stroke_train$TARGET
res_lr <- observed_vals_lr - predicted_vals_lr
mse_lr <- mean(res_lr^2)

# Lasso Regression
predicted_vals_lasso <- predict(lasso_model, newx = X, s = optimal_lambda, type = "response")
observed_vals_lasso <- Y
res_lasso <- observed_vals_lasso - predicted_vals_lasso
mse_lasso <- mean(res_lasso^2)

# Ridge Regression
predicted_vals_ridge <- predict(ridge_model, newx = X, s = optimal_lambda, type = "response")
observed_vals_ridge <- Y
res_ridge <- observed_vals_ridge - predicted_vals_ridge
mse_ridge <- mean(res_ridge^2)

# Decision Tree Model
tree_model <- rpart(TARGET ~ ., data = stroke_train, method = "class")
predicted_vals_tree <- predict(tree_model, newdata = stroke_train, type = "prob")[,2]  # assuming TARGET = 1 is the second column
observed_vals_tree <- stroke_train$TARGET
res_tree <- observed_vals_tree - predicted_vals_tree
mse_tree <- mean(res_tree^2)

# Print MSE of each model
cat("Logistic Regression MSE:", mse_lr, "\n")
cat("Lasso Regression MSE:", mse_lasso, "\n")
cat("Ridge Regression MSE:", mse_ridge, "\n")
cat("Decision Tree MSE:", mse_tree, "\n")


```

The mean squared error (MSE) results from the different models in our analysis present a close comparison, particularly among the logistic, Lasso, and Ridge regression models, with the Decision Tree model performing slightly worse. The Logistic Regression model achieved the lowest MSE at 0.1094177, indicating it was the most accurate in predicting the target variable among the four models. This suggests that the logistic model, despite its simplicity relative to the regularized models, managed to fit the data slightly better without overfitting, as the regularization in Lasso and Ridge did not significantly enhance the model accuracy in this case.

The Lasso Regression is almost identical to that of the Logistic Regression, at 0.1094384, showing that the penalty applied to reduce the coefficients of less important predictors did not substantially improve prediction accuracy. Similarly, the Ridge Regression, which also applies a penalty but does not reduce coefficients to zero, showed a marginally higher MSE of 0.1095066. This implies that the penalty in Ridge, which aims to handle multicollinearity and reduce model complexity, was also not significantly beneficial in this context.

The Decision Tree model had the highest MSE at 0.1276622, suggesting it was less effective at predicting the target compared to the regression-based models. This could be due to the model overfitting the training data or not capturing the linear relationships as effectively as the regression models. Decision trees are typically more sensitive to the specific structure of the training data and can lead to higher variance if not properly tuned or if the data does not support the tree's split criteria well.


Now, I will plot the residuals to visualize:

```{r}

residuals_data <- data.frame(
  Observed = c(observed_vals_lr, observed_vals_lasso, observed_vals_ridge, observed_vals_tree),
  Residuals = c(res_lr, res_lasso, res_ridge, res_tree),
  Model = factor(c(rep("Logistic", length(res_lr)),
                   rep("Lasso", length(res_lasso)),
                   rep("Ridge", length(res_ridge)),
                   rep("Tree", length(res_tree))) 
                 )
)

ggplot(residuals_data, aes(x = Observed, y = Residuals, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals Plot for Logistic, Lasso, Ridge, and Tree Regression Models",
       x = "Observed Values",
       y = "Residuals") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "green", "purple", "orange")) + 
  theme(legend.title = element_blank())

```


Moving forward, lets use the Lgistic model

Now lets make predictions using the test dataset.

```{r}
names(stroke_test) <- gsub("test", "train", names(stroke_test))
print(colnames(stroke_test))

# Create model matrix for the test dataset
X_test <- model.matrix(~ . - 1, data = stroke_test)

# Recheck column names to ensure they match those used in the training model
print(colnames(X_test))

# Predict using the logistic regression model
predicted_probs <- predict(model, newdata = stroke_test, type = "response")

# Create a data frame for visualization
predictions <- data.frame(Probability = predicted_probs)


ggplot(predictions, aes(x = Probability)) +
  geom_histogram(bins = 50, fill = "blue", color = "black") +
  ggtitle("Histogram of Predicted Probabilities") +
  xlab("Predicted Probability of TARGET = 1") +
  ylab("Frequency") +
  theme_minimal()
```



### Model Interpretation

We therefore select the logistic regression model as the model to interpret for our project.


```{r}


coefficients <- coef(model)

print(summary(model))


```

Our logistic regression model reveals a mixture of significant and non-significant predictors. Notably, the model adjusts for various clinical and demographic factors.

Significant variables like age, hasIVTPA, and MRS discharge score cleaned have direct implications on our model's ability to predict the TARGET. 

Age and hasIVTPA (intravenous thrombolysis treatment) are inversely related to the likelihood of the TARGET, indicating that younger ages and those not receiving IVTPA might have different outcomes relative to the base case. On the other hand, an increase in MRS discharge score, which measures the degree of disability or dependence in daily activities, positively influences the TARGET, suggesting higher scores (more disability) are associated with the outcome.

Interestingly, several dummy variables representing racial and ethnic categories were significant but showed no consistent trend in influence across the groups, highlighting the complexity of how these socio-demographic factors interact with medical outcomes.

Notable among the findings is the variable 'dummy_hypertension_N', which significantly predicts the TARGET when hypertension is absent, reflecting a strong protective effect against the condition modeled. Conversely, variables like dummy_hyperlipidemia_N and other chronic conditions displayed strong positive associations with the TARGET, suggesting that these conditions might increase the likelihood of the outcome.

The coefficients for log-transformed BMI and log-transformed Length of stay hours were also significant, suggesting that as these values increase, they have a discernible impact on the likelihood of the TARGET, though the relationship with BMI was negative, indicating a complex interaction potentially mediated by other factors.

The presence of 'NA' across numerous coefficients indicates issues of multicollinearity or perfect separation, where some predictors perfectly predict the outcome, thus are not included in the final model due to redundancy or statistical indefiniteness.

